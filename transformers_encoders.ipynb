{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkZ7bh0eXfGJCHkjFZIXrP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdityaJ9082/ML/blob/main/transformers_encoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "yfTutQdO_pau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIZaf0IW7zr2"
      },
      "outputs": [],
      "source": [
        "d_model=512\n",
        "num_heads=8\n",
        "max_sequence_length=200\n",
        "drop_prob=0.1\n",
        "ffn_hidden=2048\n",
        "num_layers=5\n",
        "batch_size=30\n",
        "\n",
        "# encoder=Encoder(d_model,num_heads,max_sequence_length,drop_prob,ffn_hidden,num_layers,batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rnp_s1Fr_og6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,d_model,ffn_hidden,num_heads,drop_prob,num_layers):\n",
        "    super().__init__()\n",
        "    self.layers=nn.Sequential(*[EncoderLayer(d_model,ffn_hidden,num_heads,drop_prob)\n",
        "                              for _ in range(num_layers)])\n",
        "  def forward(self,x):\n",
        "    x=self.layers(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "dSuwCsN1_iVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.activation import MultiheadAttention\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self,d_model,ffn_hidden,num_heads,drop_prob,num_layers):\n",
        "    super(EncoderLayer,self).__init__()\n",
        "    self.attention=MultiHeadAttention(d_model=d_model,num_heads=num_heads)\n",
        "    self.norm1=LayerNormalization(params_shape=[d_model])\n",
        "    self.drop1=nn.Dropout(p=drop_prob)\n",
        "    self.ffn=Positionwisefeedforward(d_model=d_model,hidden=ffn_hidden,drop_prob=drop_prob)\n",
        "    self.norm2=LayerNormalization(params_shape=[d_model])\n",
        "    self.drop2=nn.Dropout(p=drop_prob)\n",
        "\n",
        "  def forward(self,x):#30,200,512\n",
        "    residual_x=x\n",
        "    x=self.attention(x,mask=None)\n",
        "    x=self.drop1(x)\n",
        "    x=self.norm1(x+residual_x)\n",
        "    residual_x=x\n",
        "    x=self.ffn(x)\n",
        "    x=self.drop2(x)\n",
        "    x=self.norm2(x+residual_x)\n",
        "    return x#30,200,512\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uQXKOzBjr3oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# # x=np.ones()\n",
        "# x=np.ones(shape=(2,3,4))\n",
        "# x\n",
        "# x.shape#2,3,4#bac=tch_size,msl,emb_dim\n",
        "# x.T.shape#4,3,2\n",
        "# x.transpose(-1,-2).shape#2,4,3----->transpose for key vector"
      ],
      "metadata": {
        "id": "Nwcz13q7vwH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q,k,v,mask=None):\n",
        "  d_k=q.size()[-1]#q is batch_size,msl,emb_dim(for each head)\n",
        "  scaled=torch.matmul(q,k.transpose(-1,-2))/np.sqrt(d_k)#msl,msl----- / by dk for stable learning for basically we normalize the product so that during BP if we have some large values in our\n",
        "  #gradient steps .so these values have to be normalized so that we can take stable or even steps during training\n",
        "  if mask is not None:\n",
        "    scaled=scaled+mask\n",
        "  attention=F.softmax(scaled,dim=-1)\n",
        "  values=torch.matmul(attention,v)\n",
        "  return values,attention"
      ],
      "metadata": {
        "id": "uXQ_xZA_2EDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_model,num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model#512\n",
        "    self.qkv_layer=nn.Linear(d_model,3*d_model)\n",
        "    self.num_heads=num_heads#8\n",
        "    self.head_dims=d_model//num_heads#64\n",
        "    self.linear_layer=nn.Linear(d_model,d_model)\n",
        "\n",
        "  def forward(self,x,mask=None):\n",
        "    batch_size,sequence_length,d_model=x.size()\n",
        "    qkv=self.qkv_layer(x)#30,200,1536\n",
        "    qkv=qkv.reshape(batch_size,sequence_length,self.num_heads,3*self.head_dims)#query_matrix*3#30,200,8,192\n",
        "    qkv=qkv.permute(0,2,1,3)#30,8,200,192\n",
        "    q,k,v=qkv.chunk(3,dim=-1)#30,8,200,64 each\n",
        "    values,attention=scaled_dot_product(q,k,v,mask)#values=30,8,200,64 attention=30,8,200,200\n",
        "    values=values.reshape(batch_size,sequence_length,self.num_heads*self.head_dims)\n",
        "    out=self.linear_layer(values)\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "ESe5Eu3UQu_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self,parameters_shape,eps=1e-5):\n",
        "    super().__init__()\n",
        "    self.parameters_shape=parameters_shape\n",
        "    self.eps=eps\n",
        "    self.gamma=nn.Parameter(torch.ones(self.parameter_shapes))#[512]\n",
        "    self.beta=nn.Parameter(torch.zeros(self.parameter_shapes))#[512]\n",
        "\n",
        "  def forward(self,inputs):#30,300,512\n",
        "    dims=[-(i+1) for i in range(len(self.parameters_shape))]#[-1]\n",
        "    mean=inputs.mean(dim=dims,keepdim=True)#30,200,1\n",
        "    var=((inputs-mean)**2).mean(dim=dims,keepdim=True)#30,200,1\n",
        "    std=(var+self.eps).sqrt()\n",
        "    y=(inputs-mean)/std #30,300,512\n",
        "    out=self.gamma*y+self.beta\n",
        "    return out"
      ],
      "metadata": {
        "id": "4hNsQeYinEqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Positionwisefeedforward(nn.Module):\n",
        "  def __init__(self,d_model,ffn_hidden,drop_prob):\n",
        "    super().__init__()\n",
        "    self.linear1=nn.Linear(d_model,ffn_hidden)#512,2048\n",
        "    self.linear2=nn.Linear(ffn_hidden,d_model)#2048,512\n",
        "    self.relu=nn.ReLU()\n",
        "    self.dropout=nn.Dropout(p=drop_prob)\n",
        "\n",
        "  def forward(self,x):#30,200,512\n",
        "    x=self.linear1(x)#30,200,2048\n",
        "    x=self.relu(x)#30,200,2048\n",
        "    x=self.dropout(x)#30,200,2048\n",
        "    x=self.linear2(x)#30,200,512\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xuq4YU_xqGCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bVxSN5rfthXP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}